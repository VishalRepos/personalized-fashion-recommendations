{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":31254,"databundleVersionId":3103714,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/uom23mlmsc229407x/dm-project-collaborative-filtering-229407x?scriptVersionId=152746300\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datetime\nfrom matplotlib import pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the data","metadata":{}},{"cell_type":"code","source":"articles = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/articles.csv\")\ncustomers = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/customers.csv\")\ntransactions = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-19T05:07:12.506864Z","iopub.execute_input":"2023-11-19T05:07:12.507604Z","iopub.status.idle":"2023-11-19T05:08:19.278998Z","shell.execute_reply.started":"2023-11-19T05:07:12.507576Z","shell.execute_reply":"2023-11-19T05:08:19.277269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collaborative Filtering","metadata":{}},{"cell_type":"markdown","source":"**Pre processing**","metadata":{}},{"cell_type":"markdown","source":"we only care about customers and the articles which are bought by the customers","metadata":{}},{"cell_type":"code","source":"transactions = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', dtype={'article_id':str})\ntransactions.drop(['sales_channel_id', 'price'], inplace=True, axis=1)\ntransactions['bought'] = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filter older transactions and articles which are not sold over n times","metadata":{}},{"cell_type":"code","source":"start_date = datetime.datetime(2020,9,1)\n# Filter transactions by date\ntransactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\ntransactions = transactions.loc[transactions[\"t_dat\"] >= start_date]\n\n# Filter transactions by number of an article has been bought\narticle_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(columns={'t_dat': 'count'})\nmost_bought_articles = article_bought_count[article_bought_count['count']>10]['article_id'].values\ntransactions = transactions[transactions['article_id'].isin(most_bought_articles)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate negative samples\nnp.random.seed(42)\n\nnegative_samples = pd.DataFrame({\n    'article_id': np.random.choice(transactions.article_id.unique(), transactions.shape[0]),\n    'customer_id': np.random.choice(transactions.customer_id.unique(), transactions.shape[0]),\n    'bought': np.zeros(transactions.shape[0])\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training customer and article vector representations using the information whether a customer bought a article or not**","metadata":{}},{"cell_type":"markdown","source":"Variables","metadata":{}},{"cell_type":"code","source":"# variables\n\n# vector length to represent article and customers\nnum_components = 1000\n\n# learning rate\nlearning_rate = 0.001\n\n# lambda\nlmbda = 0.1\n\n# number of epochs\nn_epochs = 20\n\n# combine postive and negative transaction to create on data set\ntransactions = pd.concat([transactions, negative_transactions])\ncustomers = transactions.customer_id.values\narticles = transactions.article_id.values\nbought = transactions.bought.values\n\n\n# creates a dictionary. keys = customer id, value: an index/id for each customer\ncustomer_id2index = {c: i for i, c in enumerate(np.unique(customers))}\n# creates a dictionary. keys = customer id, value: an index/id for each customer\narticle_id2index = {a: i for i, a in enumerate(np.unique(articles))}\n\ntraining_indices = None\ncustomers_latent_matrix = None\narticles_latent_matrix = None\n\nn_samples = transactions.shape[0]\n\n# Initialize latent matrices - n_components vector representation for customers and articles\n# (n_cust_uniq X n_components) \ncustomers_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(customers)), num_components))\n# same for articles\narticles_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(articles)), num_components))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train","metadata":{}},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    print('Epoch: {}'.format(epoch))\n    training_indices = np.arange(n_samples)\n\n    # Shuffle training samples and follow stochastic gradient descent\n    np.random.shuffle(training_indices)\n    # __sdg__()\n    \n    # tqdm is a progress bar\n    for idx in tqdm(training_indices):\n        # Get the current sample\n        customer_id = customers[idx]\n        article_id = articles[idx]\n        print(bought)\n        print(idx)\n        print(type(bought))\n        bought_val = bought[idx]\n\n        # Get the index of the user and the article\n        customer_index = customer_id2index[customer_id]\n        article_index = article_id2index[article_id]\n\n        ## Compute the prediction and the error\n        # get the dot product of two vectors\n        prediction = np.dot(customers_latent_matrix[customer_index], articles_latent_matrix[article_index])\n        # trim/clip the prediction vlaue\n        # if prediction < 0: prediction = 0\n        # elif prediction > 0 : prediction = 1\n        # else prediction = prediction\n        prediction = np.clip(prediction, 0, 1)\n        error = (bought_val - prediction) # error\n\n        # Update latent factors in terms of the learning rate and the observed error\n        # c = c + alpha x (e x a - lamda x c)\n        customers_latent_matrix[customer_index] += learning_rate * \\\n                                (error * articles_latent_matrix[article_index] - \\\n                                 lmbda * customers_latent_matrix[customer_index])\n        # a = a + alpha x (e x c - lamda x a)\n        articles_latent_matrix[article_index] += learning_rate * \\\n                                (error * customers_latent_matrix[customer_index] - \\\n                                 lmbda * articles_latent_matrix[article_index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions**","metadata":{}},{"cell_type":"code","source":"customers = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').customer_id.unique()\n\nrecommendations = []\n\n# Compute similarity matrix (cosine)\n# ouputs a matrix of size: (self.articles_latent_matrix, self.articles_latent_matrix)\n# (i, j) th item represents similarity score of ith and jjth item. 1 is good. 0 is bad. \nsimilarity_matrix = cosine_similarityarticles_latent_matrix,articles_latent_matrix, dense_output=False)\n\n# Convert similarity matrix into a matrix containing the 12 most similar items' index for each item\n# out size : (len(articles), 12)\n# the has closesst 12 atrcles for each article\n# make 12 a variable\nsimilarity_matrix = np.argsort(similarity_matrix, axis=1)\nsimilarity_matrix = similarity_matrix[:, -12:]\n\n# Get default recommendation (time decay popularity)\n# Calculate time decaying popularity\n# Calculate time decaying popularity. This leads to items bought more recently having more weight in the popularity list.\n# In simple words, item A bought 5 times on the first day of the train period is inferior than item B bought 4 times on the last day of the train period.\npositive_transactions['pop_factor'] = positive_transactions['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\ntransactions_by_article = positive_transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\ndefault_recommendation = transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]\n# default_recommendation = default_recommendation()\n\n# Group articles by user and articles to compute the number of times each article has been bought by each user\n# transactions_by_customer -> all customer, atricle combinations and their counts avialable in positive transactions\ntransactions_by_customer = positive_transactions[['customer_id', 'article_id', 'bought']].groupby(['customer_id', 'article_id']).count().reset_index()\n# most bought article for each customer\nmost_bought_article = transactions_by_customer.loc[transactions_by_customer.groupby('customer_id').bought.idxmax()]['article_id'].values\n\n# Make predictions\nfor customer in tqdm(customers):\n    try:\n        rec_aux1 = []\n        rec_aux2 = []\n        aux = []\n\n        # Retrieve the most bought article by customer\n        user_most_bought_article_id = most_bought_article[self.customer_id2index[customer]]\n\n        # Using the similarity matrix, get the 6 most similar articles\n        rec_aux1 = self.articles[similarity_matrix[self.article_id2index[user_most_bought_article_id]]]\n        # Return the half of the default recommendation\n        rec_aux2 =  default_recommendation\n\n        # Merge half of both recommendation lists\n        for rec_idx in range(6):\n            aux.append(rec_aux2[rec_idx])\n            aux.append(rec_aux1[rec_idx])\n\n        recommendations.append(' '.join(aux))\n    except:\n        # Return the default recommendation\n        recommendations.append(' '.join(default_recommendation))\n\nprediction_df =  pd.DataFrame({'customer_id': customers, 'prediction': recommendations})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}